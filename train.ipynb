{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for kaggle competition: text emotion classification.\n",
    "* In this notebook file, it contains both training and testing stages.\n",
    "* I tried to use powerful pretrained language model to extract meaningful sentence features, and then fine-tuning classification task by pretrained LM and classifer.\n",
    "* If you want to run this file, please install transformers package provided by HuggingFace: ```pip install transformers``` .\n",
    "* This code is based on Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read the prepared training and testing data in csv format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train_data.csv\")\n",
    "test_data = pd.read_csv(\"test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>identification</th>\n",
       "      <th>emotion</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0x29e452</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "      <td>Huge Respect🖒 @JohnnyVegasReal talking about l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0x2b3819</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "      <td>Yoooo we hit all our monthly goals with the ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0x2a2acc</td>\n",
       "      <td>train</td>\n",
       "      <td>trust</td>\n",
       "      <td>@KIDSNTS @PICU_BCH @uhbcomms @BWCHBoss Well do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0x2a8830</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "      <td>Come join @ambushman27 on #PUBG while he striv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0x20b21d</td>\n",
       "      <td>train</td>\n",
       "      <td>anticipation</td>\n",
       "      <td>@fanshixieen2014 Blessings!My #strength little...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455558</th>\n",
       "      <td>1455558</td>\n",
       "      <td>0x227e25</td>\n",
       "      <td>train</td>\n",
       "      <td>disgust</td>\n",
       "      <td>@BBCBreaking Such an inspirational talented pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455559</th>\n",
       "      <td>1455559</td>\n",
       "      <td>0x293813</td>\n",
       "      <td>train</td>\n",
       "      <td>sadness</td>\n",
       "      <td>And still #libtards won't get off the guy's ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455560</th>\n",
       "      <td>1455560</td>\n",
       "      <td>0x1e1a7e</td>\n",
       "      <td>train</td>\n",
       "      <td>joy</td>\n",
       "      <td>When you sow #seeds of service or hospitality ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455561</th>\n",
       "      <td>1455561</td>\n",
       "      <td>0x2156a5</td>\n",
       "      <td>train</td>\n",
       "      <td>trust</td>\n",
       "      <td>@lorettalrose Will you be displaying some &lt;LH&gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455562</th>\n",
       "      <td>1455562</td>\n",
       "      <td>0x2bb9d2</td>\n",
       "      <td>train</td>\n",
       "      <td>trust</td>\n",
       "      <td>Lord, I &lt;LH&gt; in you.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1455563 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0  tweet_id identification       emotion  \\\n",
       "0                 0  0x29e452          train           joy   \n",
       "1                 1  0x2b3819          train           joy   \n",
       "2                 2  0x2a2acc          train         trust   \n",
       "3                 3  0x2a8830          train           joy   \n",
       "4                 4  0x20b21d          train  anticipation   \n",
       "...             ...       ...            ...           ...   \n",
       "1455558     1455558  0x227e25          train       disgust   \n",
       "1455559     1455559  0x293813          train       sadness   \n",
       "1455560     1455560  0x1e1a7e          train           joy   \n",
       "1455561     1455561  0x2156a5          train         trust   \n",
       "1455562     1455562  0x2bb9d2          train         trust   \n",
       "\n",
       "                                                      text  \n",
       "0        Huge Respect🖒 @JohnnyVegasReal talking about l...  \n",
       "1        Yoooo we hit all our monthly goals with the ne...  \n",
       "2        @KIDSNTS @PICU_BCH @uhbcomms @BWCHBoss Well do...  \n",
       "3        Come join @ambushman27 on #PUBG while he striv...  \n",
       "4        @fanshixieen2014 Blessings!My #strength little...  \n",
       "...                                                    ...  \n",
       "1455558  @BBCBreaking Such an inspirational talented pe...  \n",
       "1455559  And still #libtards won't get off the guy's ba...  \n",
       "1455560  When you sow #seeds of service or hospitality ...  \n",
       "1455561  @lorettalrose Will you be displaying some <LH>...  \n",
       "1455562                               Lord, I <LH> in you.  \n",
       "\n",
       "[1455563 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We have to map string type emotion labels to int type of label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Emotion_label = {\n",
    "        'sadness': 0,\n",
    "        'disgust': 1,\n",
    "        'anticipation': 2,\n",
    "        'joy': 3,\n",
    "        'trust': 4,\n",
    "        'anger': 5,\n",
    "        'fear': 6,\n",
    "        'surprise': 7\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_to_label(emotion):\n",
    "    label = Emotion_label[emotion]\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"emotion\"] = train_data[\"emotion\"].apply(emotion_to_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check if the conversion is successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 2, 0, 1, 6, 7, 5])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"emotion\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Noted]\n",
    "##### I have tried to convert the emojis back to the word by \"emoji\" package and train the model again, so that tokenizer can recognize the wrod that emoji represent to and use the additional information to train the model. However, After training, the result on Kaggle only improve from 0.57297 to 0.57338 on public leaderboard. The improvement is lower than my expectation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "def demoji(text):\n",
    "    return emoji.demojize(text, delimiters=(\"\", \"\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"text\"] = train_data[\"text\"].apply(demoji)\n",
    "test_data[\"text\"] = test_data[\"text\"].apply(demoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = train_data[\"text\"].values\n",
    "train_labels = train_data[\"emotion\"].values\n",
    "test_texts = test_data[\"text\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This part is to split the training set to training and validation data, but in the end, after I fine-tuning my hyperparameters,  I use full training set to train my model in order to get better performance on Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#train_texts, val_texts, train_labels, val_labels = train_test_split(train_text, train_label, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of training data:  1455563\n"
     ]
    }
   ],
   "source": [
    "print(\"# of training data: \", len(train_texts))\n",
    "#print(\"# of validation data: \", len(val_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I use the Autoclass in transformers because it supports various pretrained language models, such as standard BERT and RoBERTa, and the implementation is simple.\n",
    "* In my trials, I have tried bert-base-uncased and roberta-base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23f3e71c83704cae9bfe944a38bb50f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Downloading', max=501200538, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "models = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=8)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The input of BERT tokenizer has to be a list of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_texts = list(train_texts)\n",
    "#val_texts = list(val_texts)\n",
    "test_texts = list(test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use pytorch Dataset class to define my own customized dataset called TweetDataset, for the later usage of Trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class TweetsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenizer, texts, labels):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=64, verbose = True)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whether using GPU:  True\n",
      "GPU name:  Tesla V100-SXM2-32GB\n"
     ]
    }
   ],
   "source": [
    "print(\"Whether using GPU: \", torch.cuda.is_available())\n",
    "print(\"GPU name: \", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This is an example to show how the tokenizer output a sentence. The output includes \"input_ids\" and \"attention_mask\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 725, 11797, 33106, 6569, 25448, 10659, 787, 39249, 846, 3733, 281, 17105, 1686, 59, 2], [0, 725, 11797, 33106, 6569, 25448, 10659, 787, 39249, 846, 3733, 281, 17105, 1686, 59, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"Huge Respect🖒 @JohnnyVegasReal talking about\", \"Huge Respect🖒 @JohnnyVegasReal talking about\"],\n",
    "          truncation=True, padding=True, max_length=64, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Build training dataset by the predefined TweetsDataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TweetsDataset(tokenizer, train_texts, list(train_labels))\n",
    "#val_dataset = TweetsDataset(tokenizer, val_texts, list(val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use \"TrainingArguments\" and \"Trainer\" to train the downstream classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./results_roberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: daniel091444 (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.12<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">./results_roberta</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/daniel091444/huggingface\" target=\"_blank\">https://wandb.ai/daniel091444/huggingface</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/daniel091444/huggingface/runs/1d3473ex\" target=\"_blank\">https://wandb.ai/daniel091444/huggingface/runs/1d3473ex</a><br/>\n",
       "                Run data is saved locally in <code>/home/daniel094144/Daniel/2020DM/wandb/run-20201207_205142-1d3473ex</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='17058' max='17058' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17058/17058 4:03:55, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.390715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.075018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.013678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.978613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.951293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.916831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.878918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.867628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.863633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.856300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.844597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.797908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.773238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.766628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.764751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.757305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.756352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=17058, training_loss=0.8968088038432555)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results_roberta',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=256,  # batch size per device during training\n",
    "    #per_device_eval_batch_size=128,   # batch size for evaluation\n",
    "    warmup_steps=2000,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs_roberta',            # directory for storing logs\n",
    "    logging_steps=1000, \n",
    "    save_steps=10000\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=models,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    #eval_dataset=val_dataset           # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('./DM_roberta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Testing stage\n",
    "* In this phase, I load the trained model and predict the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('DM_roberta', num_labels=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Please note that the customized Dataset is different from training Dataset because now the testing data doesn't contain emotion labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetsTestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenizer, texts):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=64, verbose = True)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TweetsTestDataset(tokenizer, test_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make prediction by DataLoader to load our testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "cpu = torch.device('cpu')\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "pred_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "    \n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        pred_labels.append(outputs[0].argmax(dim=1).to(cpu))\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "pred_labels = torch.cat(pred_labels, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The dictionary that used for converting int type of class back to string type of class, in order to match the format of  submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Emotion_label = {\n",
    "        0: 'sadness',\n",
    "        1: 'disgust',\n",
    "        2: 'anticipation',\n",
    "        3: 'joy',\n",
    "        4: 'trust',\n",
    "        5: 'anger',\n",
    "        6: 'fear',\n",
    "        7: 'surprise'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = [Emotion_label[int(label)] for label in pred_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pd.DataFrame(data = emotions, columns = ['emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411967</th>\n",
       "      <td>surprise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411968</th>\n",
       "      <td>anticipation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411969</th>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411970</th>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411971</th>\n",
       "      <td>joy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>411972 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             emotion\n",
       "0            sadness\n",
       "1            sadness\n",
       "2            sadness\n",
       "3            sadness\n",
       "4                joy\n",
       "...              ...\n",
       "411967      surprise\n",
       "411968  anticipation\n",
       "411969       sadness\n",
       "411970           joy\n",
       "411971           joy\n",
       "\n",
       "[411972 rows x 1 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>identification</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0x28cc61</td>\n",
       "      <td>test</td>\n",
       "      <td>@Habbo I've seen two separate colours of the e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0x2db41f</td>\n",
       "      <td>test</td>\n",
       "      <td>@FoxNews @KellyannePolls No serious self respe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0x2466f6</td>\n",
       "      <td>test</td>\n",
       "      <td>Looking for a new car, and it says 1 lady owne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0x23f9e9</td>\n",
       "      <td>test</td>\n",
       "      <td>@cineworld “only the brave” just out and fount...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0x1fb4e1</td>\n",
       "      <td>test</td>\n",
       "      <td>Felt like total dog 💩 going into open gym and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411967</th>\n",
       "      <td>411967</td>\n",
       "      <td>0x2c4dc2</td>\n",
       "      <td>test</td>\n",
       "      <td>6 year old walks in astounded. Mum! Look how b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411968</th>\n",
       "      <td>411968</td>\n",
       "      <td>0x31be7c</td>\n",
       "      <td>test</td>\n",
       "      <td>Only one week to go until the #inspiringvolunt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411969</th>\n",
       "      <td>411969</td>\n",
       "      <td>0x1ca58e</td>\n",
       "      <td>test</td>\n",
       "      <td>I just got caught up with the manga for \"My He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411970</th>\n",
       "      <td>411970</td>\n",
       "      <td>0x35c8ba</td>\n",
       "      <td>test</td>\n",
       "      <td>Speak only when spoken to and make hot ass mus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411971</th>\n",
       "      <td>411971</td>\n",
       "      <td>0x1d941b</td>\n",
       "      <td>test</td>\n",
       "      <td>Know what you want and go for it. Fuck everyon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>411972 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  tweet_id identification  \\\n",
       "0                0  0x28cc61           test   \n",
       "1                1  0x2db41f           test   \n",
       "2                2  0x2466f6           test   \n",
       "3                3  0x23f9e9           test   \n",
       "4                4  0x1fb4e1           test   \n",
       "...            ...       ...            ...   \n",
       "411967      411967  0x2c4dc2           test   \n",
       "411968      411968  0x31be7c           test   \n",
       "411969      411969  0x1ca58e           test   \n",
       "411970      411970  0x35c8ba           test   \n",
       "411971      411971  0x1d941b           test   \n",
       "\n",
       "                                                     text  \n",
       "0       @Habbo I've seen two separate colours of the e...  \n",
       "1       @FoxNews @KellyannePolls No serious self respe...  \n",
       "2       Looking for a new car, and it says 1 lady owne...  \n",
       "3       @cineworld “only the brave” just out and fount...  \n",
       "4       Felt like total dog 💩 going into open gym and ...  \n",
       "...                                                   ...  \n",
       "411967  6 year old walks in astounded. Mum! Look how b...  \n",
       "411968  Only one week to go until the #inspiringvolunt...  \n",
       "411969  I just got caught up with the manga for \"My He...  \n",
       "411970  Speak only when spoken to and make hot ass mus...  \n",
       "411971  Know what you want and go for it. Fuck everyon...  \n",
       "\n",
       "[411972 rows x 4 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[\"id\"] = test_data[\"tweet_id\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>identification</th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0x28cc61</td>\n",
       "      <td>test</td>\n",
       "      <td>@Habbo I've seen two separate colours of the e...</td>\n",
       "      <td>0x28cc61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0x2db41f</td>\n",
       "      <td>test</td>\n",
       "      <td>@FoxNews @KellyannePolls No serious self respe...</td>\n",
       "      <td>0x2db41f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0x2466f6</td>\n",
       "      <td>test</td>\n",
       "      <td>Looking for a new car, and it says 1 lady owne...</td>\n",
       "      <td>0x2466f6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0x23f9e9</td>\n",
       "      <td>test</td>\n",
       "      <td>@cineworld “only the brave” just out and fount...</td>\n",
       "      <td>0x23f9e9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0x1fb4e1</td>\n",
       "      <td>test</td>\n",
       "      <td>Felt like total dog 💩 going into open gym and ...</td>\n",
       "      <td>0x1fb4e1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411967</th>\n",
       "      <td>411967</td>\n",
       "      <td>0x2c4dc2</td>\n",
       "      <td>test</td>\n",
       "      <td>6 year old walks in astounded. Mum! Look how b...</td>\n",
       "      <td>0x2c4dc2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411968</th>\n",
       "      <td>411968</td>\n",
       "      <td>0x31be7c</td>\n",
       "      <td>test</td>\n",
       "      <td>Only one week to go until the #inspiringvolunt...</td>\n",
       "      <td>0x31be7c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411969</th>\n",
       "      <td>411969</td>\n",
       "      <td>0x1ca58e</td>\n",
       "      <td>test</td>\n",
       "      <td>I just got caught up with the manga for \"My He...</td>\n",
       "      <td>0x1ca58e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411970</th>\n",
       "      <td>411970</td>\n",
       "      <td>0x35c8ba</td>\n",
       "      <td>test</td>\n",
       "      <td>Speak only when spoken to and make hot ass mus...</td>\n",
       "      <td>0x35c8ba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411971</th>\n",
       "      <td>411971</td>\n",
       "      <td>0x1d941b</td>\n",
       "      <td>test</td>\n",
       "      <td>Know what you want and go for it. Fuck everyon...</td>\n",
       "      <td>0x1d941b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>411972 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  tweet_id identification  \\\n",
       "0                0  0x28cc61           test   \n",
       "1                1  0x2db41f           test   \n",
       "2                2  0x2466f6           test   \n",
       "3                3  0x23f9e9           test   \n",
       "4                4  0x1fb4e1           test   \n",
       "...            ...       ...            ...   \n",
       "411967      411967  0x2c4dc2           test   \n",
       "411968      411968  0x31be7c           test   \n",
       "411969      411969  0x1ca58e           test   \n",
       "411970      411970  0x35c8ba           test   \n",
       "411971      411971  0x1d941b           test   \n",
       "\n",
       "                                                     text        id  \n",
       "0       @Habbo I've seen two separate colours of the e...  0x28cc61  \n",
       "1       @FoxNews @KellyannePolls No serious self respe...  0x2db41f  \n",
       "2       Looking for a new car, and it says 1 lady owne...  0x2466f6  \n",
       "3       @cineworld “only the brave” just out and fount...  0x23f9e9  \n",
       "4       Felt like total dog 💩 going into open gym and ...  0x1fb4e1  \n",
       "...                                                   ...       ...  \n",
       "411967  6 year old walks in astounded. Mum! Look how b...  0x2c4dc2  \n",
       "411968  Only one week to go until the #inspiringvolunt...  0x31be7c  \n",
       "411969  I just got caught up with the manga for \"My He...  0x1ca58e  \n",
       "411970  Speak only when spoken to and make hot ass mus...  0x35c8ba  \n",
       "411971  Know what you want and go for it. Fuck everyon...  0x1d941b  \n",
       "\n",
       "[411972 rows x 5 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Concatenate the \"id\" and \"predicted emotion\" to csv file for final submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([test_data[\"id\"], pred], axis = 1).to_csv(\"submission2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "1. In this competition, I didn't do lots of feature engineering because BERT tokenizer have already set and BERT has amazing ability to extract sentence features by attention mechenism.\n",
    "2. The one feature engineering method I've tried is to map the emojis back to word, so that BERT tokenizer wouldn't tokenize emojis to <UNK>. However, the score isn't increase a lot.\n",
    "3. Compared BERT with RoBERTa, RoBERTa has better performance on the same hyperparameter setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
